# templates/ai-chat/apps/api/src/ai/ollama.py.hbs import os import json import
asyncio import logging from typing import AsyncGenerator, List, Dict, Any import
httpx from .base import LocalAIProvider, ChatMessage, AIProviderError,
ModelNotFoundError logger = logging.getLogger(__name__) class
OllamaProvider(LocalAIProvider): """Ollama local AI provider implementation"""
def __init__( self, url: str = None, default_model: str = "{{ai.providers.ollama.defaultModel}}",
available_models: List[str] = None ): url = url or os.getenv("OLLAMA_URL", "{{ai.providers.ollama.url}}")
available_models = available_models or
{{json ai.providers.ollama.models}}

super().__init__(url, default_model, available_models) self.timeout = 300.0 # 5
minutes for model operations async def chat( self, messages: List[ChatMessage],
model: str, **kwargs ) -> str: """Generate chat completion using Ollama""" try:
# Validate model if not self.validate_model(model): available_models = await
self.list_models() if model not in available_models: raise
ModelNotFoundError(f"Model {model} not available in Ollama") # Format request
request_data = { "model": model, "messages": self.format_messages(messages),
"stream": False } # Add optional parameters if "temperature" in kwargs:
request_data["options"] = request_data.get("options", {})
request_data["options"]["temperature"] = kwargs["temperature"] if "max_tokens"
in kwargs: request_data["options"] = request_data.get("options", {})
request_data["options"]["num_predict"] = kwargs["max_tokens"]
logger.debug(f"Ollama chat request: {json.dumps(request_data, indent=2)}") async
with httpx.AsyncClient(timeout=self.timeout) as client: response = await
client.post( f"{self.url}/api/chat", json=request_data )
response.raise_for_status() data = response.json() content = data.get("message",
{}).get("content", "") if not content: logger.warning(f"Empty response from
Ollama for model {model}") return "I apologize, but I couldn't generate a
response. Please try again." return content except httpx.HTTPStatusError as e:
logger.error(f"Ollama HTTP error: {e.response.status_code} - {e.response.text}")
raise AIProviderError(f"Ollama request failed: {e.response.status_code}") except
httpx.RequestError as e: logger.error(f"Ollama connection error: {str(e)}")
raise AIProviderError(f"Failed to connect to Ollama: {str(e)}") except Exception
as e: logger.error(f"Ollama chat error: {str(e)}") raise
AIProviderError(f"Ollama chat failed: {str(e)}") async def chat_stream( self,
messages: List[ChatMessage], model: str, **kwargs ) -> AsyncGenerator[str,
None]: """Stream chat completion using Ollama""" try: # Validate model if not
self.validate_model(model): available_models = await self.list_models() if model
not in available_models: raise ModelNotFoundError(f"Model {model} not available
in Ollama") # Format request request_data = { "model": model, "messages":
self.format_messages(messages), "stream": True } # Add optional parameters if
"temperature" in kwargs: request_data["options"] = request_data.get("options",
{}) request_data["options"]["temperature"] = kwargs["temperature"] if
"max_tokens" in kwargs: request_data["options"] = request_data.get("options",
{}) request_data["options"]["num_predict"] = kwargs["max_tokens"]
logger.debug(f"Ollama stream request: {json.dumps(request_data, indent=2)}")
async with httpx.AsyncClient(timeout=self.timeout) as client: async with
client.stream( "POST", f"{self.url}/api/chat", json=request_data ) as response:
response.raise_for_status() async for line in response.aiter_lines(): if
line.strip(): try: data = json.loads(line) # Check for errors if "error" in
data: logger.error(f"Ollama stream error: {data['error']}") break # Extract
content if "message" in data and "content" in data["message"]: content =
data["message"]["content"] if content: yield content # Check if done if
data.get("done", False): break except json.JSONDecodeError as e:
logger.warning(f"Failed to parse Ollama response line: {line}") continue except
httpx.HTTPStatusError as e: logger.error(f"Ollama HTTP error:
{e.response.status_code} - {e.response.text}") raise AIProviderError(f"Ollama
stream failed: {e.response.status_code}") except httpx.RequestError as e:
logger.error(f"Ollama connection error: {str(e)}") raise
AIProviderError(f"Failed to connect to Ollama: {str(e)}") except Exception as e:
logger.error(f"Ollama stream error: {str(e)}") raise AIProviderError(f"Ollama
stream failed: {str(e)}") async def list_models(self) -> List[str]: """List
available models from Ollama""" try: async with httpx.AsyncClient(timeout=30.0)
as client: response = await client.get(f"{self.url}/api/tags")
response.raise_for_status() data = response.json() models = [model["name"] for
model in data.get("models", [])] logger.debug(f"Ollama available models:
{models}") return models except httpx.HTTPStatusError as e:
logger.error(f"Ollama list models HTTP error: {e.response.status_code}") return
[] except httpx.RequestError as e: logger.error(f"Ollama connection error:
{str(e)}") return [] except Exception as e: logger.error(f"Failed to list Ollama
models: {str(e)}") return [] async def load_model(self, model: str) -> bool:
"""Load/pull a model in Ollama""" try: logger.info(f"Loading Ollama model:
{model}") async with httpx.AsyncClient(timeout=600.0) as client: # 10 minutes
for model download response = await client.post( f"{self.url}/api/pull",
json={"name": model} ) response.raise_for_status() # The response is streamed,
so we need to read it async for line in response.aiter_lines(): if line.strip():
try: data = json.loads(line) # Log progress if "status" in data:
logger.info(f"Model {model} pull status: {data['status']}") # Check if completed
if data.get("status") == "success" or "successfully" in data.get("status",
"").lower(): logger.info(f"Successfully loaded model: {model}") return True
except json.JSONDecodeError: continue return True except httpx.HTTPStatusError
as e: logger.error(f"Failed to load model {model}: {e.response.status_code}")
return False except Exception as e: logger.error(f"Error loading model {model}:
{str(e)}") return False async def health_check(self) -> bool: """Check if Ollama
is healthy""" try: async with httpx.AsyncClient(timeout=10.0) as client:
response = await client.get(f"{self.url}/api/tags") return response.status_code
== 200 except Exception as e: logger.debug(f"Ollama health check failed:
{str(e)}") return False async def get_model_info(self, model: str) -> Dict[str,
Any]: """Get detailed information about a model""" try: async with
httpx.AsyncClient(timeout=30.0) as client: response = await client.post(
f"{self.url}/api/show", json={"name": model} ) if response.status_code == 200:
data = response.json() return { "name": model, "provider": "ollama", "details":
data.get("details", {}), "parameters": data.get("parameters", {}), "template":
data.get("template", ""), "available": True } else: return { "name": model,
"provider": "ollama", "available": False, "error": f"HTTP
{response.status_code}" } except Exception as e: logger.error(f"Failed to get
model info for {model}: {str(e)}") return { "name": model, "provider": "ollama",
"available": False, "error": str(e) }