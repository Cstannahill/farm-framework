import React, { useState, useEffect, useRef } from 'react';
import { useStreamingChat } from '../hooks/useStreamingChat';
import { useAIModels } from '../hooks/useAIModels';
import MessageList from './MessageList';
import MessageInput from './MessageInput';
import TypingIndicator from './TypingIndicator';
import ModelSelector from './ModelSelector';
import type { ChatMessage } from '../types/ai';

interface ChatWindowProps {
  className?: string;
}

export default function ChatWindow({ className = '' }: ChatWindowProps) {
  const [selectedProvider, setSelectedProvider] = useState<'ollama' | 'openai'>('ollama');
  const [selectedModel, setSelectedModel] = useState<string>('');
  
  const {
    messages,
    sendMessage,
    isStreaming,
    clearMessages,
    error
  } = useStreamingChat({
    provider: selectedProvider,
    model: selectedModel,
  });
  
  const { data: models, isLoading: modelsLoading } = useAIModels(selectedProvider);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  
  // Auto-scroll to bottom when new messages arrive
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);
  
  // Set default model when models load
  useEffect(() => {
    if (models && models.length > 0 && !selectedModel) {
      const defaultModel = models.find(m => 
        m.name.includes('llama3.1') || m.name.includes('gpt-3.5-turbo')
      ) || models[0];
      setSelectedModel(defaultModel.name);
    }
  }, [models, selectedModel]);
  
  const handleSendMessage = async (content: string) => {
    if (!selectedModel) {
      alert('Please select a model first');
      return;
    }
    
    await sendMessage(content, {
      model: selectedModel,
      provider: selectedProvider,
      temperature: 0.7,
      max_tokens: 1000
    });
  };
  
  const handleClearChat = () => {
    if (window.confirm('Are you sure you want to clear the chat history?')) {
      clearMessages();
    }
  };
  
  const handleProviderChange = (provider: 'ollama' | 'openai') => {
    setSelectedProvider(provider);
    setSelectedModel(''); // Reset model when provider changes
  };
  
  return (
    <div className={`flex flex-col h-full bg-white rounded-lg shadow-lg ${className}`}>
      {/* Header */}
      <div className="flex items-center justify-between p-4 border-b border-gray-200">
        <div className="flex items-center space-x-4">
          <h2 className="text-xl font-semibold text-gray-800">
            {{projectName}} Chat
          </h2>
          <div className="flex items-center space-x-2">
            <span className="text-sm text-gray-500">Provider:</span>
            <select
              value={selectedProvider}
              onChange={(e) => handleProviderChange(e.target.value as 'ollama' | 'openai')}
              className="text-sm border rounded px-2 py-1 focus:outline-none focus:ring-2 focus:ring-blue-500"
            >
              <option value="ollama">Ollama (Local)</option>
              <option value="openai">OpenAI (Cloud)</option>
            </select>
          </div>
        </div>
        
        <div className="flex items-center space-x-2">
          <ModelSelector
            provider={selectedProvider}
            selectedModel={selectedModel}
            onModelChange={setSelectedModel}
            models={models || []}
            loading={modelsLoading}
          />
          
          <button
            onClick={handleClearChat}
            className="px-3 py-1 text-sm text-gray-600 hover:text-gray-800 border border-gray-300 rounded hover:bg-gray-50 transition-colors"
            disabled={messages.length === 0}
          >
            Clear
          </button>
        </div>
      </div>
      
      {/* Error Display */}
      {error && (
        <div className="p-4 bg-red-50 border-b border-red-200">
          <div className="flex items-center">
            <div className="flex-shrink-0">
              <svg className="h-5 w-5 text-red-400" viewBox="0 0 20 20" fill="currentColor">
                <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" />
              </svg>
            </div>
            <div className="ml-3">
              <p className="text-sm text-red-700">
                {error}
              </p>
            </div>
          </div>
        </div>
      )}
      
      {/* Messages Area */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.length === 0 ? (
          <div className="text-center py-12">
            <div className="text-gray-400 mb-4">
              <svg className="mx-auto h-12 w-12" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z" />
              </svg>
            </div>
            <p className="text-gray-500 text-lg">Start a conversation</p>
            <p className="text-gray-400 text-sm mt-2">
              Type a message below to chat with the AI
            </p>
          </div>
        ) : (
          <>
            <MessageList messages={messages} />
            {isStreaming && <TypingIndicator />}
          </>
        )}
        <div ref={messagesEndRef} />
      </div>
      
      {/* Input Area */}
      <div className="border-t border-gray-200">
        <MessageInput
          onSendMessage={handleSendMessage}
          disabled={isStreaming || !selectedModel}
          placeholder={
            !selectedModel 
              ? "Select a model to start chatting..." 
              : "Type your message..."
          }
        />
      </div>
    </div>
  );
}