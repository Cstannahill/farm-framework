# templates/ai-chat/apps/api/src/ai/ollama.py.hbs
import os
import json
import asyncio
import logging
from typing import AsyncGenerator, List, Dict, Any

import httpx

from .base import LocalAIProvider, ChatMessage, AIProviderError, ModelNotFoundError

logger = logging.getLogger(__name__)


class OllamaProvider(LocalAIProvider):
    """Ollama local AI provider implementation"""

    def __init__(
        self,
        url: str = None,
        default_model: str = "{{ai.providers.ollama.defaultModel}}",
        available_models: List[str] = None,
    ):
        url = url or os.getenv("OLLAMA_URL", "{{ai.providers.ollama.url}}")
        # 🔧 UNESCAPED JSON  ↓↓↓
        available_models = available_models or {{{json ai.providers.ollama.models}}}

        super().__init__(url, default_model, available_models)
        self.timeout = 300.0  # 5 min operations

    # ═════════════════ chat (single) ═════════════════
    async def chat(
        self,
        messages: List[ChatMessage],
        model: str,
        **kwargs,
    ) -> str:
        """Generate chat completion using Ollama."""
        try:
            if not self.validate_model(model) and model not in await self.list_models():
                raise ModelNotFoundError(f"Model {model} not available in Ollama")

            req = {
                "model": model,
                "messages": self.format_messages(messages),
                "stream": False,
            }

            if "temperature" in kwargs:
                req.setdefault("options", {})["temperature"] = kwargs["temperature"]
            if "max_tokens" in kwargs:
                req.setdefault("options", {})["num_predict"] = kwargs["max_tokens"]

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.post(f"{self.url}/api/chat", json=req)
                resp.raise_for_status()
                data = resp.json()

            content = data.get("message", {}).get("content", "")
            if not content:
                logger.warning("Empty Ollama response")
                return (
                    "I’m sorry, but I couldn’t generate a response. Please try again."
                )
            return content

        except httpx.HTTPStatusError as e:
            raise AIProviderError(f"Ollama request failed: HTTP {e.response.status_code}") from e
        except httpx.RequestError as e:
            raise AIProviderError(f"Connection error: {str(e)}") from e
        except Exception as e:
            raise AIProviderError(f"Ollama chat failed: {str(e)}") from e

    # ═════════════════ chat (stream) ═════════════════
    async def chat_stream(
        self,
        messages: List[ChatMessage],
        model: str,
        **kwargs,
    ) -> AsyncGenerator[str, None]:
        """Stream chat completion."""
        try:
            if not self.validate_model(model) and model not in await self.list_models():
                raise ModelNotFoundError(f"Model {model} not available in Ollama")

            req = {
                "model": model,
                "messages": self.format_messages(messages),
                "stream": True,
            }
            if "temperature" in kwargs:
                req.setdefault("options", {})["temperature"] = kwargs["temperature"]
            if "max_tokens" in kwargs:
                req.setdefault("options", {})["num_predict"] = kwargs["max_tokens"]

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST", f"{self.url}/api/chat", json=req
                ) as resp:
                    resp.raise_for_status()
                    async for line in resp.aiter_lines():
                        if not line.strip():
                            continue
                        try:
                            chunk = json.loads(line)
                        except json.JSONDecodeError:
                            logger.debug(f"Bad JSON line: {line}")
                            continue

                        if chunk.get("error"):
                            logger.error(f"Ollama stream error: {chunk['error']}")
                            break
                        if "message" in chunk and "content" in chunk["message"]:
                            yield chunk["message"]["content"]
                        if chunk.get("done"):
                            break

        except httpx.HTTPStatusError as e:
            raise AIProviderError(f"Ollama stream failed: HTTP {e.response.status_code}") from e
        except httpx.RequestError as e:
            raise AIProviderError(f"Connection error: {str(e)}") from e
        except Exception as e:
            raise AIProviderError(f"Ollama stream failed: {str(e)}") from e

    # ═════════════════ utilities ═════════════════════
    async def list_models(self) -> List[str]:
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                r = await client.get(f"{self.url}/api/tags")
                r.raise_for_status()
                return [m["name"] for m in r.json().get("models", [])]
        except Exception as e:
            logger.error(f"List models failed: {str(e)}")
            return []

    async def load_model(self, model: str) -> bool:
        """Pull a model (can take time)."""
        try:
            async with httpx.AsyncClient(timeout=600.0) as client:
                async with client.stream(
                    "POST", f"{self.url}/api/pull", json={"name": model}
                ) as resp:
                    resp.raise_for_status()
                    async for line in resp.aiter_lines():
                        if not line.strip():
                            continue
                        try:
                            data = json.loads(line)
                            if data.get("status") == "success" or "successfully" in data.get("status", "").lower():
                                return True
                        except json.JSONDecodeError:
                            continue
            return True
        except Exception as e:
            logger.error(f"Pull model {model} failed: {str(e)}")
            return False

    async def health_check(self) -> bool:
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                return (await client.get(f"{self.url}/api/tags")).status_code == 200
        except Exception:
            return False

    async def get_model_info(self, model: str) -> Dict[str, Any]:
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                r = await client.post(f"{self.url}/api/show", json={"name": model})
            if r.status_code == 200:
                data = r.json()
                return {
                    "name": model,
                    "provider": "ollama",
                    "details": data.get("details", {}),
                    "parameters": data.get("parameters", {}),
                    "template": data.get("template", ""),
                    "available": True,
                }
            return {
                "name": model,
                "provider": "ollama",
                "available": False,
                "error": f"HTTP {r.status_code}",
            }
        except Exception as e:
            logger.error(f"Get model info failed: {str(e)}")
            return {
                "name": model,
                "provider": "ollama",
                "available": False,
                "error": str(e),
            }