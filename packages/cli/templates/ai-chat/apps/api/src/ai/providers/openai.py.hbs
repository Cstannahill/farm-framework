# templates/ai-chat/apps/api/src/ai/openai.py.hbs
import os
import json
import logging
from typing import AsyncGenerator, List, Dict, Any

import httpx

from .base import (
    CloudAIProvider,
    ChatMessage,
    AIProviderError,
    ModelNotFoundError,
)

logger = logging.getLogger(__name__)


class OpenAIProvider(CloudAIProvider):
    """OpenAI API provider implementation"""

    def __init__(
        self,
        api_key: str = None,
        default_model: str = "{{ai.providers.openai.defaultModel}}",
        available_models: List[str] = None,
    ):
        api_key = api_key or os.getenv("OPENAI_API_KEY")

        {{#if ai.providers.openai.models}}
        # User‑supplied models
        available_models = available_models or {{{json ai.providers.openai.models}}}
        {{else}}
        # Fallback to empty list so Python stays valid
        available_models = available_models or []
        {{/if}}

        if not api_key:
            raise ValueError("OpenAI API key is required")

        super().__init__(api_key, default_model, available_models)
        self.base_url = "https://api.openai.com/v1"
        self.timeout = 120.0  # 2 min per request

    # ════════════════════════ helpers ════════════════════════
    def _get_headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    # ═════════════════════ text‑completions ══════════════════
    async def chat(
        self,
        messages: List[ChatMessage],
        model: str,
        **kwargs,
    ) -> str:
        """Single‑shot chat completion."""
        try:
            if not self.validate_model(model):
                raise ModelNotFoundError(f"Model {model} not available in OpenAI")

            request = {
                "model": model,
                "messages": self.format_messages(messages),
            }

            # optional params
            for param in (
                "temperature",
                "max_tokens",
                "top_p",
                "frequency_penalty",
                "presence_penalty",
                "stop",
            ):
                if param in kwargs:
                    request[param] = kwargs[param]

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                r = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers=self._get_headers(),
                    json=request,
                )
                r.raise_for_status()
                data = r.json()

            if data.get("choices"):
                return data["choices"][0]["message"]["content"] or ""

            logger.warning("Empty response from OpenAI")
            return (
                "I’m sorry, but I couldn’t generate a response. Please try again later."
            )

        # ----- error handling -----
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)
        except httpx.RequestError as e:
            raise AIProviderError(f"Connection error: {str(e)}") from e
        except Exception as e:
            raise AIProviderError(f"OpenAI chat failed: {str(e)}") from e

    # ═════════════════════ streaming completions ═════════════
    async def chat_stream(
        self,
        messages: List[ChatMessage],
        model: str,
        **kwargs,
    ) -> AsyncGenerator[str, None]:
        """Stream chat completion."""
        if not self.validate_model(model):
            raise ModelNotFoundError(f"Model {model} not available in OpenAI")

        request = {
            "model": model,
            "messages": self.format_messages(messages),
            "stream": True,
        }

        for param in (
            "temperature",
            "max_tokens",
            "top_p",
            "frequency_penalty",
            "presence_penalty",
            "stop",
        ):
            if param in kwargs:
                request[param] = kwargs[param]

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST",
                    f"{self.base_url}/chat/completions",
                    headers=self._get_headers(),
                    json=request,
                ) as resp:
                    resp.raise_for_status()
                    async for line in resp.aiter_lines():
                        if line.startswith("data: "):
                            payload = line[6:].strip()
                            if payload == "[DONE]":
                                break
                            try:
                                chunk = json.loads(payload)
                                delta = chunk["choices"][0]["delta"]
                                if "content" in delta:
                                    yield delta["content"]
                            except json.JSONDecodeError:
                                logger.debug(f"Bad JSON chunk: {payload}")
                                continue
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)
        except httpx.RequestError as e:
            raise AIProviderError(f"Connection error: {str(e)}") from e
        except Exception as e:
            raise AIProviderError(f"OpenAI stream failed: {str(e)}") from e

    # ═════════════════════ metadata / health ═════════════════
    async def list_models(self) -> List[str]:
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                r = await client.get(
                    f"{self.base_url}/models", headers=self._get_headers()
                )
                r.raise_for_status()
                data = r.json()
            return [m["id"] for m in data.get("data", [])]
        except Exception as e:
            logger.error(f"List models failed: {str(e)}")
            return self.available_models

    async def health_check(self) -> bool:
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                r = await client.get(
                    f"{self.base_url}/models", headers=self._get_headers()
                )
            return r.status_code == 200
        except Exception:
            return False

    # ═════════════════════ utils ═════════════════════════════
    async def _handle_http_error(self, err: httpx.HTTPStatusError):
        status = err.response.status_code
        text = err.response.text
        logger.error(f"OpenAI HTTP {status}: {text}")

        if status == 401:
            raise AIProviderError("Invalid OpenAI API key") from err
        if status == 429:
            raise AIProviderError("OpenAI rate limit exceeded") from err
        if status == 400:
            try:
                msg = err.response.json().get("error", {}).get("message", "Bad request")
            except Exception:  # pragma: no cover
                msg = "Bad request"
            raise AIProviderError(f"OpenAI error: {msg}") from err

        raise AIProviderError(f"OpenAI request failed: HTTP {status}") from err
